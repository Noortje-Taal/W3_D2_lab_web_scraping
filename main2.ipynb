{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "\n",
    "#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris Banes',\n",
       " 'Yangshun Tay',\n",
       " 'Franck Nijhof',\n",
       " 'Micah Lee',\n",
       " 'Ana Schwendler',\n",
       " 'Alon Zakai',\n",
       " 'Paulus Schoutsen',\n",
       " 'Asim Aslam',\n",
       " 'Samuel Colvin',\n",
       " 'CrazyMax',\n",
       " 'Mr.doob',\n",
       " 'Nico Schlömer',\n",
       " '文翼',\n",
       " 'Miek Gieben',\n",
       " 'Henrik Rydgård',\n",
       " 'Tanner',\n",
       " 'Kyle Roach',\n",
       " 'Saúl Ibarra Corretgé',\n",
       " 'James Agnew',\n",
       " 'bluss',\n",
       " 'Rob Rix',\n",
       " 'Bo-Yi Wu',\n",
       " 'Sascha Grunert',\n",
       " 'Cezar Augusto',\n",
       " 'Teppei Fukuda']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "#your code\n",
    "names = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "rows = [name.find_all('a') for name in names] \n",
    "just_names = [x.text.strip().split(\"\\n\") for row in rows for x in row] #dubble for loop because rows is list in list\n",
    "\n",
    "list_names = [x for name in just_names for x in name]\n",
    "list_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['facebookresearch / detectron2',\n",
       " 'Linzaer / Ultra-Light-Fast-Generic-Face-Detector-1MB',\n",
       " 'gto76 / python-cheatsheet',\n",
       " '0xInfection / Awesome-WAF',\n",
       " 'donnemartin / system-design-primer',\n",
       " 'facebookresearch / hydra',\n",
       " 'ansible / ansible',\n",
       " 'isocpp / CppCoreGuidelines',\n",
       " 'milesial / Pytorch-UNet',\n",
       " 'deepinsight / insightface',\n",
       " '649453932 / Chinese-Text-Classification-Pytorch',\n",
       " 'MasoniteFramework / masonite',\n",
       " 'awslabs / serverless-application-model',\n",
       " 'ymcui / Chinese-BERT-wwm',\n",
       " '3b1b / manim',\n",
       " 'kubeflow / pipelines',\n",
       " 'pytorch / fairseq',\n",
       " 'Unity-Technologies / ml-agents',\n",
       " 'pandas-dev / pandas',\n",
       " 'williamFalcon / pytorch-lightning',\n",
       " 'Delgan / loguru',\n",
       " 'AlessandroZ / LaZagne',\n",
       " 'wemake-services / wemake-python-styleguide',\n",
       " 'nccgroup / demiguise',\n",
       " 'awslabs / aws-cloudformation-templates']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "\n",
    "# import juiste content in soup format \n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# waar naar op zoek\n",
    "repos = soup.find_all('h1',{'class':'h3 lh-condensed'})\n",
    "rows = [repo.find_all('a') for repo in repos]\n",
    "\n",
    "# clean\n",
    "just_text = [x.text.strip().split(\"\\n\") for row in rows for x in row]\n",
    "just_text_2 = [[x.strip() for x in text if x] for text in just_text]\n",
    "\n",
    "# make a correct list \n",
    "list_repos = [x + ' ' + y for (x,y) in just_text_2] # you can say for x,y in just_text_2 \n",
    "list_repos                                             # than say how x en y should be print within the list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/wikimedia-button.png',\n",
       " '/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# waar naar op zoek \n",
    "image_link_list = [img.get('src') for img in soup.find_all('img')] \n",
    "image_link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=918851926',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sd.wikipedia.org/wiki/%D8%A7%D8%B1%DA%99',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\") \n",
    "\n",
    "# what do I want \n",
    "href_list = [a.get('href') for a in soup.find_all('a')]\n",
    "\n",
    "# clear\n",
    "href_list[0] = 'hi'\n",
    "\n",
    "# making correct list\n",
    "link_list = [href for href in href_list if href.startswith('http')]\n",
    "link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# wat wil je hebben\n",
    "names = soup.find_all('h3',{'class':'title'})\n",
    "rows = [name.find_all('a') for name in names]\n",
    "\n",
    "# filter \n",
    "just_names = [x.text.strip().split(\"\\n\") for row in rows for x in row]\n",
    "\n",
    "list_just_names = [x for name in just_names for x in name]\n",
    "list_just_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_name</th>\n",
       "      <th>date_time</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CENTRAL ALASKA</td>\n",
       "      <td>2019-10-16 09:25</td>\n",
       "      <td>62.58 N</td>\n",
       "      <td>156.31 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZORES-CAPE ST. VINCENT RIDGE</td>\n",
       "      <td>2019-10-16 09:24</td>\n",
       "      <td>36.40 N</td>\n",
       "      <td>11.20 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "      <td>2019-10-16 09:26</td>\n",
       "      <td>38.41 N</td>\n",
       "      <td>41.17 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANDREANOF ISLANDS, ALEUTIAN IS.</td>\n",
       "      <td>2019-10-16 09:13</td>\n",
       "      <td>51.34 N</td>\n",
       "      <td>178.73 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2019-10-16 08:27</td>\n",
       "      <td>36.17 N</td>\n",
       "      <td>120.29 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2019-10-16 07:55</td>\n",
       "      <td>36.64 N</td>\n",
       "      <td>121.26 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NIAS REGION, INDONESIA</td>\n",
       "      <td>2019-10-16 08:02</td>\n",
       "      <td>1.26 N</td>\n",
       "      <td>97.11 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>2019-10-16 07:41</td>\n",
       "      <td>37.34 N</td>\n",
       "      <td>30.76 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SERBIA</td>\n",
       "      <td>2019-10-16 08:35</td>\n",
       "      <td>43.15 N</td>\n",
       "      <td>20.12 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2019-10-16 07:05</td>\n",
       "      <td>37.52 N</td>\n",
       "      <td>118.89 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "      <td>2019-10-16 07:08</td>\n",
       "      <td>21.19 S</td>\n",
       "      <td>68.82 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "      <td>2019-10-16 05:56</td>\n",
       "      <td>35.83 N</td>\n",
       "      <td>117.62 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>THAILAND-LAOS BORDER REGION</td>\n",
       "      <td>2019-10-16 05:55</td>\n",
       "      <td>17.72 N</td>\n",
       "      <td>101.70 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>VIRGIN ISLANDS REGION</td>\n",
       "      <td>2019-10-16 07:50</td>\n",
       "      <td>19.46 N</td>\n",
       "      <td>64.68 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DODECANESE IS.-TURKEY BORDER REG</td>\n",
       "      <td>2019-10-16 05:41</td>\n",
       "      <td>36.10 N</td>\n",
       "      <td>27.70 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "      <td>2019-10-16 05:20</td>\n",
       "      <td>42.95 N</td>\n",
       "      <td>12.69 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>2019-10-16 05:12</td>\n",
       "      <td>36.71 N</td>\n",
       "      <td>98.42 W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "      <td>2019-10-16 04:50</td>\n",
       "      <td>40.51 N</td>\n",
       "      <td>27.11 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CRETE, GREECE</td>\n",
       "      <td>2019-10-16 06:51</td>\n",
       "      <td>35.05 N</td>\n",
       "      <td>23.25 E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NORTHERN ALASKA</td>\n",
       "      <td>2019-10-16 03:58</td>\n",
       "      <td>69.61 N</td>\n",
       "      <td>144.24 W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          region_name         date_time  longitude    latitude\n",
       "0                      CENTRAL ALASKA  2019-10-16 09:25  62.58 N    156.31 W  \n",
       "1       AZORES-CAPE ST. VINCENT RIDGE  2019-10-16 09:24  36.40 N     11.20 W  \n",
       "2                      EASTERN TURKEY  2019-10-16 09:26  38.41 N     41.17 E  \n",
       "3     ANDREANOF ISLANDS, ALEUTIAN IS.  2019-10-16 09:13  51.34 N    178.73 W  \n",
       "4                  CENTRAL CALIFORNIA  2019-10-16 08:27  36.17 N    120.29 W  \n",
       "5                  CENTRAL CALIFORNIA  2019-10-16 07:55  36.64 N    121.26 W  \n",
       "6              NIAS REGION, INDONESIA  2019-10-16 08:02   1.26 N     97.11 E  \n",
       "7                      WESTERN TURKEY  2019-10-16 07:41  37.34 N     30.76 E  \n",
       "8                              SERBIA  2019-10-16 08:35  43.15 N     20.12 E  \n",
       "9                  CENTRAL CALIFORNIA  2019-10-16 07:05  37.52 N    118.89 W  \n",
       "10                    TARAPACA, CHILE  2019-10-16 07:08  21.19 S     68.82 W  \n",
       "11                 CENTRAL CALIFORNIA  2019-10-16 05:56  35.83 N    117.62 W  \n",
       "12        THAILAND-LAOS BORDER REGION  2019-10-16 05:55  17.72 N    101.70 E  \n",
       "13              VIRGIN ISLANDS REGION  2019-10-16 07:50  19.46 N     64.68 W  \n",
       "14   DODECANESE IS.-TURKEY BORDER REG  2019-10-16 05:41  36.10 N     27.70 E  \n",
       "15                      CENTRAL ITALY  2019-10-16 05:20  42.95 N     12.69 E  \n",
       "16                           OKLAHOMA  2019-10-16 05:12  36.71 N     98.42 W  \n",
       "17                     WESTERN TURKEY  2019-10-16 04:50  40.51 N     27.11 E  \n",
       "18                      CRETE, GREECE  2019-10-16 06:51  35.05 N     23.25 E  \n",
       "19                    NORTHERN ALASKA  2019-10-16 03:58  69.61 N    144.24 W  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\") \n",
    "\n",
    "# what do you want in soup\n",
    "table = soup.find_all('tbody', {'id': 'tbody'})[0]\n",
    "rows = table.find_all('tr')\n",
    "rows_2 = [row.find_all('td') for row in rows]\n",
    "\n",
    "# get text in correct format (list in list)\n",
    "text = [[x.text for x in row] for row in rows_2] # hier zeg je doe [x.text for x in row] in een list in een list\n",
    "                                                 # anders raak je de structure kwijt \n",
    "# make dataframe \n",
    "df = pd.DataFrame(text)\n",
    "\n",
    "# cleaning within dataframe \n",
    "df.drop([0, 1, 2, 3], axis=1, inplace=True) # drop empty rows\n",
    "\n",
    "cols = ['longitude1', 'longitude2', 'latitude1', 'latitude2', 'del', 'del1', 'del2', 'region_name', 'date_time']\n",
    "df.columns = cols # name columns \n",
    "\n",
    "df.drop(['del', 'del1', 'del2'], axis=1, inplace=True) # drop unnessecary cols\n",
    "\n",
    "df['longitude'] = df['longitude1'] + df['longitude2'] # combine columns \n",
    "df['latitude'] = df['latitude1'] + df['latitude2']\n",
    "\n",
    "df.drop(['longitude1', 'longitude2', 'latitude1', 'latitude2'], axis=1, inplace=True) # drop unnessecary\n",
    "\n",
    "# print tot 20 \n",
    "df.head(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oerlikon Digital Hub Hackathon']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# what do i need\n",
    "events = soup.find_all(\"h5\", {'class', 'card-title'})\n",
    "\n",
    "# cleaning\n",
    "just_text = [event.text.strip().split(\"\\n\") for event in events]\n",
    "\n",
    "# make a list\n",
    "list_events = [x for text in just_text for x in text]\n",
    "list_events\n",
    "\n",
    "# it is still not the same content as within the inspect on the website, so i could not finish it\n",
    "# because of this, i just made a list of all the upcoming events (this is only just 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#i do not have Twitter, so i did one of the bonus questions instead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "#i do not have Twitter, so i did another bonus questions instead "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# what do i want \n",
    "info = soup.find_all('div',{'class': 'grid-row dgu-topics'})\n",
    "rows = [i.find_all('a') for i in info]\n",
    "\n",
    "# cleaning \n",
    "just_text = [x.text.strip().split(\"\\n\") for row in rows for x in row]\n",
    "\n",
    "# make list \n",
    "list_datasets = [x for text in just_text for x in text]\n",
    "list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Primary Country[clarification needed]</th>\n",
       "      <th>TotalCountries[a]</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>% of the World population</th>\n",
       "      <th>Language familyBranch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin (language family)[9]</td>\n",
       "      <td>China</td>\n",
       "      <td>13</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>Spain</td>\n",
       "      <td>31</td>\n",
       "      <td>460</td>\n",
       "      <td>5.974</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>137</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-EuropeanGermanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi[10]</td>\n",
       "      <td>India</td>\n",
       "      <td>4</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>4</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>Portugal</td>\n",
       "      <td>15</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>Russian Federation</td>\n",
       "      <td>19</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-EuropeanBalto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>Japan</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662</td>\n",
       "      <td>JaponicJapanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[11]</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>2</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>India</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank                       Language Primary Country[clarification needed]  \\\n",
       "0    1  Mandarin (language family)[9]                                 China   \n",
       "1    2                        Spanish                                 Spain   \n",
       "2    3                        English                        United Kingdom   \n",
       "3    4                      Hindi[10]                                 India   \n",
       "4    5                        Bengali                            Bangladesh   \n",
       "5    6                     Portuguese                              Portugal   \n",
       "6    7                        Russian                    Russian Federation   \n",
       "7    8                       Japanese                                 Japan   \n",
       "8    9            Western Punjabi[11]                              Pakistan   \n",
       "9   10                        Marathi                                 India   \n",
       "\n",
       "  TotalCountries[a] Speakers(millions) % of the World population  \\\n",
       "0                13                918                    11.922   \n",
       "1                31                460                     5.974   \n",
       "2               137                379                     4.922   \n",
       "3                 4                341                     4.429   \n",
       "4                 4                228                     2.961   \n",
       "5                15                221                     2.870   \n",
       "6                19                154                     2.000   \n",
       "7                 2                128                     1.662   \n",
       "8                 2               92.7                     1.204   \n",
       "9                 1               83.1                     1.079   \n",
       "\n",
       "       Language familyBranch  \n",
       "0        Sino-TibetanSinitic  \n",
       "1       Indo-EuropeanRomance  \n",
       "2      Indo-EuropeanGermanic  \n",
       "3    Indo-EuropeanIndo-Aryan  \n",
       "4    Indo-EuropeanIndo-Aryan  \n",
       "5       Indo-EuropeanRomance  \n",
       "6  Indo-EuropeanBalto-Slavic  \n",
       "7            JaponicJapanese  \n",
       "8    Indo-EuropeanIndo-Aryan  \n",
       "9    Indo-EuropeanIndo-Aryan  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "# what do i want \n",
    "language = soup.find_all('table',{'class': 'wikitable sortable'})\n",
    "rows = [lang.find_all('tr') for lang in language] \n",
    "\n",
    "# cleaning\n",
    "just_text = [x.text.strip().split(\"\\n\") for row in rows for x in row]\n",
    "just_text_clean = [[x for x in text if x] for text in just_text]\n",
    "del just_text_clean[0][6] \n",
    "\n",
    "# make df top 10 \n",
    "data_cols = just_text_clean[0]\n",
    "data_cols\n",
    "data_rows = just_text_clean[1:]\n",
    "\n",
    "top_10_languages = pd.DataFrame(data_rows, columns = data_cols)\n",
    "top_10_languages.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
